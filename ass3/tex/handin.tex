\documentclass[a4paper,10pt]{article}
\usepackage[a4paper, total={210mm,297mm}, left=20mm, right=20mm, top=20mm, bottom=20mm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{parskip}
\renewcommand\thesection{\arabic{section}}
\renewcommand\thesubsection{\arabic{section}.\alph{subsection}}
\renewcommand\thesubsubsection{\arabic{section}.\alph{subsection}.\roman{subsubsection}}

%opening
\title{Assignment 3 \\ Optimizing CUDA programs \\Programming Massively Parallel Hardware }
\author{Mads Thoudahl /qmh332}

\begin{document}

\maketitle

\vfill

All functions are implemented in the following five files: \\
\texttt{test.cu} - testruns and output printing. \\
\texttt{hostlib.cu.h} - implementation of helper functions, CPU versions, timings, and preparations of device calls. \\
\texttt{devlib.cu.h} - implementation of device configuration, version handling, and device kernel calls. \\
\texttt{devkernels.cu.h} - implementation of the actual device kernels used. \\
\texttt{Makefile} - few modifications compared to handout version.

Parameters can be tweaked the following 2 places: TILE\_SIZE, and BLOCK\_SIZE as \texttt{\#define}'s in \texttt{devkernels.cu.h} and the integers used to call the \texttt{test*}-functions which determines the problem sizes used in the calculations.

the files are zipped and the commands needed when running the files are:
\texttt{make run} and \texttt{make clean}. Enjoy.

All timings are done in a fashion where memory transfers to and from device are excluded.

\vfill
\section{Matrix transposition}

From the program output underneath, we see that matrix transposition is extremely fast on the GPU as compared to the CPU. 
This example shows that a transpose taking almost 7 seconds on a CPU takes around 3 milliseconds on the optimal version on the GPU.
This is a speed improvement comparable to the number of cores (2880) on the GPU device, a 3 orders of magnitude speedup.

Comparing the two GPU implementations, the tiling technique, means that the coalesced memory operations provides a 2.0 speedup.

\vfill

\begin{verbatim}
ASSIGNMENT3 TASK1: MATRIX TRANSPOSITION

Transpose Matrix sized 8192 x 8192 running times
CPU:              6964901 microsecs. 
GPU naïve:           6145 microsecs. --  VALID
GPU optimized:       3068 microsecs. --  VALID

Giga MemoryOPerations per second:
CPU:                0.019 Gmop/s.
GPU naïve:         21.842 Gmop/s.
GPU optimized:     43.748 Gmop/s.

This is a speedup of    2.00, for tile and coalesced mem accesses on GPU.
... and a speedup of 1133.24, for GPU naive compared to CPU.
... and a speedup of 2269.44, for GPU opt   compared to CPU.
\end{verbatim}


\subsection{Matrix transposition CPU} implementation reduced from hostlib.cu.h:
\begin{verbatim}
template<class T> int transpose_cpu(int rows, int cols, T *m_in, T *m_out ) {
    for (int row=0; row<rows_in; row++){
        for (int col=0; col<cols_in; col++) {
            m_out[col*rows_in+row] = m_in[row*cols_in+col];
        }
    }
}
\end{verbatim}

\vfill

\subsection{Matrix transposition in OpenMP - omitted}
focusing on the techniques, using CUDA as the tool.

\vfill

\subsection{Matrix transposition GPU - Naïve} implementations reduced from devkernels.cu.h:
\begin{verbatim}
template<class T> __global__ void
transpose_naive_kernel( cols_out, cols_in, T* d_in, T* d_out ) {
    const unsigned int xid = blockIdx.x * blockDim.x + threadIdx.x;
    const unsigned int yid = blockIdx.y * blockDim.y + threadIdx.y;

    int read_idx  = yid * cols_in + xid;
    int write_idx = xid * cols_out + yid;
 
    if ( (yid < cols_out) & (xid < cols_in) ) {
        d_out[write_idx] = d_in[read_idx];
    }
}
\end{verbatim}

\vfill

\subsection{Matrix transposition GPU - Optimal} implementations reduced from devkernels.cu.h:
\begin{verbatim}
template<class T> __global__ void
transpose_opt_kernel( int cols_out, int cols_in, T* d_in, T* d_out ){
    __shared__ T tile_mem[TILE_SIZE][TILE_SIZE+1]; // MEMORY BANK FIX ??
 
    // calculate indexing into global array
    unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;
    unsigned int y = blockIdx.y * blockDim.x + threadIdx.y;

    if ((y < cols_out) & (x < cols_in)) {
        tile_mem[threadIdx.y][threadIdx.x]  = d_in[y * cols_in + x];
    }
    __syncthreads();
    x = blockIdx.y * blockDim.x + threadIdx.x;  // transpose global indexing
    y = blockIdx.x * blockDim.x + threadIdx.y;

    if ((y < cols_in) & (x < cols_out)) {
        d_out[y * cols_out + x] = tile_mem[threadIdx.x][threadIdx.y];
    }
}
\end{verbatim}
\vfill

\newpage
\section{Demonstrating the usefullness of matrix transpositions}

consulting the output of the program, we see that the second implementation runs approximately 4.5 times faster than the first implementation, despite the extra memory operations, proving the concept of accessing memory in a coalesced manor.

in the following subsections we try and discover the reasons, and display the techniques used to achieve this performance.


\begin{verbatim}
ASSIGNMENT3 TASK2: MATRIX TRANSPOSITION AS PREPROCESSING

Matrix accfun on size 393216 x 64. Running times:
CPU:               457792 microsecs. 
GPU first:          18567 microsecs. --  VALID
GPU second:          4109 microsecs. --  VALID

This is a speedup of    4.52, for second compared to first on GPU.
... and a speedup of   24.65, for GPU first  compared to CPU.
... and a speedup of  111.38, for GPU second compared to CPU.
\end{verbatim}

\vfill

\subsection{Reason about top-level parallellism of the code}

Original matrix accumulation function
\begin{verbatim}
 1 for i from 0 to N-1
 2     accum = A[i,0] * A[i,0]
 3     B[i,0] = accum
 4     for j from 1 to 63
 5         tmpA = A[i,j]
 6         accum = sqrt(accum) + tmpA * tmpA
 7         B[i,j] = accum
\end{verbatim}

\subsubsection{The outer loop}
Assuming that \texttt{tmpA} and \texttt{accum} are initialized and declared outside of the loop, the outer loop is not parallel because we have a loop-carried dependency in the \texttt{accum} variable. If parallellized, all threads running the outer loop wants to write to the same \texttt{accum} variable. 
This is remedied by privatizing the \texttt{accum} variable, declaring it inside the loop, or equivalently performing an array-expansion, that is providing one for each thread \texttt{i} running the outer loop, and distributing the loop.

Rewritten matrix accumulation function:
\begin{verbatim}
 0 float* accum[N];                // create N accum variables
 1 for i from 0 to N-1             // one for each parallel thread
 2     accum[i] = A[i,0] * A[i,0]  // use private accum[i]
 3     B[i,0] = accum[i]           
 4     for j from 1 to 63
 5         tmpA = A[i,j]
 6         accum[i] = sqrt(accum[i]) + tmpA * tmpA
 7         B[i,j] = accum[i]
\end{verbatim}


\subsubsection{The inner loop}
the inner loop is not parallel as we have a cross-iteration dependency in the accum variable yet again.
the accum variable is written at every iteration and read in the next iteration.
At iteration j, we read A[i,j] and accum, accum has a \texttt{True Dependency} as it is written in previous iteration (j-1), and in iteration (j) we again write to accum.

We may try to check if loop interchange is an option, but as the accum is written in the outer loop and read at the inner loop, thus it is not.
This behavior is not secure, it indicates a problem. 
This dendency cannot be resolved.

\subsubsection{Inner loop rewrite I}
As we investigate, we find the problem really is that we apply the sqrt() operation to accum, which is unary, thus does not fulfil the demands of a monoid, being a binary operator.

\subsubsection{Inner loop rewrite II}
If line 6 is rewritten:   \texttt{accum += A[i,j] * A[i,j]} then it is seen that this can be rewritten into 
\begin{verbatim}
 // i being index of outer loop
 tmp  = map sqrt() A[i]
 B[i] = scan( (+) 0 tmpA )
\end{verbatim}

 
\vfill

\subsection{Bonus: OpenMP solution - Omitted}
focusing on the techniques, using CUDA as the tool.


\subsection{First CUDA version of code}
pretty much as the handed out code, although the \texttt{accum} and \texttt{tmp} variables have been privatized.
\begin{verbatim}
  1 template<class T> __global__ void
  2 mat_acc_first_kernel( int rows_in, int cols_in, T* d_in, T* d_out ) {
  3     int gid = blockIdx.x * blockDim.x + threadIdx.x;
  4     if (gid < rows_in) {
  5         T accum =  d_in[gid*cols_in] * d_in[gid*cols_in];
  6         d_out[gid*cols_in] = accum;
  7         T tmp;
  8         for (int i=1; i < cols_in; i++) {
  9             tmp    = d_in[gid * cols_in + i];
 10             accum  = sqrt(accum) + tmp*tmp;
 11             d_out[gid * cols_in + i] = accum;
 12         }
 13     }
 14 }
\end{verbatim}

\vfill

\subsection{Second version of code}


Transpose the input, and the output, making way for coherence in memory accesses.
\begin{verbatim}
 transpose<T>( rows_in, cols_in, d_in, d_in_t );
 mat_acc_second_kernel<T><<< num_blocks, block_size >>>( cols_in, rows_in, d_in_t, d_out_t );
 transpose<T>( cols_in, rows_in, d_out_t, d_out );
\end{verbatim}

Kernel code:
\begin{verbatim}
  1 template<class T> __global__ void
  2 mat_acc_second_kernel( int rows_in, int cols_in, T* d_in, T* d_out ) {
  3     int gid = blockIdx.x * blockDim.x + threadIdx.x;
  4     if (gid < cols_in) {
  5         T accum =  d_in[gid]*d_in[gid];
  6         T tmp;
  7         d_out[gid] = accum;
  8         for (int i=1; i < rows_in; i++) {
  9             tmp    = d_in[gid + cols_in * i];
 10             accum  = sqrt(accum) + tmp*tmp;
 11             d_out[gid + cols_in * i] = accum;
 12         }
 13     }
 14 }
\end{verbatim}

\vfill

\newpage
\section{Dense matrix multiplication}
Looking to the output of the program underneath, we see that the calculation of the matrix multiplication on two 1280x1280 matrices takes around 11.7 seconds to perform for the CPU.
This is performed so much faster by the optimal GPU version timed at 11.4 milliseconds, which yet improves the naïve GPU implementation by a 2.5x speedup.

Calculating the number of floating point operations per second, in Giga ($10^9$) Flop per second or just Gflop/s.
We see that the optimized implementation scores an amazing 367.98 Gflops/s, 3 orders of magnitude more than the serial implementation.

in the following subsections we display the techniques and implementations used to achieve this performance.

\begin{verbatim}
ASSIGNMENT3 TASK3: MATRIX MULTIPLICATION

Matrix Multiplication on (1280x1280) x (1280,1280). Timings:
CPU:             11709561 microsecs. 
GPU naïve:          28865 microsecs. --  VALID
GPU optimized:      11398 microsecs. --  VALID

Giga FLoatingpointOPerations per second:
CPU:                0.358 Gflop/s.
GPU naïve:        145.308 Gflop/s.
GPU optimized:    367.986 Gflop/s.

This is a speedup of    2.53, for second compared to first on GPU.
... and a speedup of  405.65, for GPU naive compared to CPU.
... and a speedup of 1027.24, for GPU opt   compared to CPU.
\end{verbatim}

\vfill

\subsection{Matrix multiplication CPU} implementation reduced from hostlib.cu.h. No parallellism exploited, sequential implementation:
\begin{verbatim}
  1 template<class T> int
  2 matmult_cpu( int rows_in_a, int cols_in_a, T* h_in_a, int rows_in_b, int cols_in_b, T* h_in_b, T* h_out ) {
  3     float tmp;
  4     for (int i=0 ; i<rows_in_a ; i++ ) {
  5         for (int j=0 ; j<cols_in_b ; j++) {
  6             tmp = 0;
  7             for (int k=0 ; k<cols_in_a ; k++) {
  8                 tmp += h_in_a[i*cols_in_a+k] * h_in_b[k*cols_in_b+j];
  9             }
 10             h_out[i*cols_in_b + j] = tmp;
 11         }
 12     }
 13 }
\end{verbatim}

\vfill

\subsection{Matrix multiplication in OpenMP - omitted}
focusing on the techniques, using CUDA as the tool.

\vfill 

\subsection{Matrix multiplication GPU - Naïve} implementations reduced from devkernels.cu.h: \\
using loop unroll technique in both x and y dimension, we end up with 2 outer parallel loops representing the grid of parallellism in the kernel call, thus not represented in the kernel implementation.
privatization of tmp variable.
\begin{verbatim}
  1 template<class T> __global__ void
  2 matmult_naive_kernel( int M, int U, int N, T* A, T* B, T* res ) {
  3     int j = blockIdx.x * blockDim.x + threadIdx.x;
  4     int i = blockIdx.y * blockDim.y + threadIdx.y;
  5     if ((j < N) & (i < M)) {
  6         float tmp = 0.0;
  7         for (int k=0 ; k<U ; k++) {
  8             tmp += A[i*U+k] * B[k*N+j];
  9         }
 10         res[j + N * i] = tmp;
 11     }
 12 }
\end{verbatim}

\vfill


\subsection{Matrix multiplication GPU - Optimal} implementations reduced from devkernels.cu.h.\\
same parallel exploitations as in the naïve version, and further on...\\
line 3: allocating shared memory a bit more mem is allocated than needed, to avoid memory bank conflicts.\\
line 12-20 loop unrolling in the U-dimension.
line 13-14, storing the input data in shared memory, reading from global memory in a \textit{coalesced} manor.\\
line 17 using the shared memory to calculate the output.\\
line 22 single store in global memory
\begin{verbatim}
 1 template<class T> __global__ void
 2 matmult_tile_kernel( int M, int U, int N, T* A, T* B, T* res ) {
 3     __shared__ T Ash[TILE_SIZE+1][TILE_SIZE+1], Bsh[TILE_SIZE+1][TILE_SIZE+1];
 4     int x = threadIdx.x;
 5     int y = threadIdx.y;
 6     int tile = blockDim.x; // = blockDim.y
 7 
 8     int j  = blockIdx.x * tile + x;
 9     int i  = blockIdx.y * tile + y;
10
11     float tmp = 0.0;
12     for (int kk=0 ; kk<U ; kk+=tile) {
13         Ash[y][x] = ((i<M) && ((kk+x)<U)) ? A[i*U+(kk+x)] : 0.0 ;
14         Bsh[y][x] = ((j<N) && ((kk+y)<U)) ? B[(kk+y)*N+j] : 0.0 ;
15         __syncthreads();
16         for (int k=0 ; k<tile ; k++) {
17             tmp += Ash[y][k] * Bsh[k][x];
18         }
19         __syncthreads();
20     }
21     if ((j < N) & (i < M)) {
22         res[j + N * i] = tmp;
23     }
24 }
\end{verbatim}
\vfill

\end{document}
